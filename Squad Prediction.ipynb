{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tokenization\n",
    "import six\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import tf_utils\n",
    "from albert import AlbertConfig, AlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALBertQALayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Layer computing position and is_possible for question answering task.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, start_n_top, end_n_top, initializer, dropout, **kwargs):\n",
    "        \"\"\"Constructs Summarization layer.\n",
    "        Args:\n",
    "          hidden_size: Int, the hidden size.\n",
    "          start_n_top: Beam size for span start.\n",
    "          end_n_top: Beam size for span end.\n",
    "          initializer: Initializer used for parameters.\n",
    "          dropout: float, dropout rate.\n",
    "          **kwargs: Other parameters.\n",
    "        \"\"\"\n",
    "        super(ALBertQALayer, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.start_n_top = start_n_top\n",
    "        self.end_n_top = end_n_top\n",
    "        self.initializer = initializer\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, unused_input_shapes):\n",
    "        \"\"\"Implements build() for the layer.\"\"\"\n",
    "        self.start_logits_proj_layer = tf.keras.layers.Dense(\n",
    "            units=1, kernel_initializer=self.initializer, name='start_logits/dense')\n",
    "        self.end_logits_proj_layer0 = tf.keras.layers.Dense(\n",
    "            units=self.hidden_size,\n",
    "            kernel_initializer=self.initializer,\n",
    "            activation=tf.nn.tanh,\n",
    "            name='end_logits/dense_0')\n",
    "        self.end_logits_proj_layer1 = tf.keras.layers.Dense(\n",
    "            units=1, kernel_initializer=self.initializer, name='end_logits/dense_1')\n",
    "        self.end_logits_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "            axis=-1, epsilon=1e-12, name='end_logits/LayerNorm')\n",
    "        self.answer_class_proj_layer0 = tf.keras.layers.Dense(\n",
    "            units=self.hidden_size,\n",
    "            kernel_initializer=self.initializer,\n",
    "            activation=tf.nn.tanh,\n",
    "            name='answer_class/dense_0')\n",
    "        self.answer_class_proj_layer1 = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            kernel_initializer=self.initializer,\n",
    "            use_bias=False,\n",
    "            name='answer_class/dense_1')\n",
    "        self.ans_feature_dropout = tf.keras.layers.Dropout(rate=self.dropout)\n",
    "        super(ALBertQALayer, self).build(unused_input_shapes)\n",
    "\n",
    "    def __call__(self,\n",
    "                 sequence_output,\n",
    "                 p_mask,\n",
    "                 cls_index,\n",
    "                 start_positions=None,\n",
    "                 **kwargs):\n",
    "        inputs = tf_utils.pack_inputs(\n",
    "            [sequence_output, p_mask, cls_index, start_positions])\n",
    "        return super(ALBertQALayer, self).__call__(inputs, **kwargs)\n",
    "\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"Implements call() for the layer.\"\"\"\n",
    "        unpacked_inputs = tf_utils.unpack_inputs(inputs)\n",
    "        sequence_output = unpacked_inputs[0]\n",
    "        p_mask = unpacked_inputs[1]\n",
    "        cls_index = unpacked_inputs[2]\n",
    "        start_positions = unpacked_inputs[3]\n",
    "\n",
    "        _, seq_len, _ = sequence_output.shape.as_list()\n",
    "        sequence_output = tf.transpose(sequence_output, [1, 0, 2])\n",
    "\n",
    "        start_logits = self.start_logits_proj_layer(sequence_output)\n",
    "        start_logits = tf.transpose(tf.squeeze(start_logits, -1), [1, 0])\n",
    "        start_logits_masked = start_logits * (1 - p_mask) - 1e30 * p_mask\n",
    "        start_log_probs = tf.nn.log_softmax(start_logits_masked, -1)\n",
    "\n",
    "        if kwargs.get(\"training\", False):\n",
    "            # during training, compute the end logits based on the\n",
    "            # ground truth of the start position\n",
    "            start_positions = tf.reshape(start_positions, [-1])\n",
    "            start_index = tf.one_hot(start_positions, depth=seq_len, axis=-1,\n",
    "                                     dtype=tf.float32)\n",
    "            start_features = tf.einsum(\n",
    "                'lbh,bl->bh', sequence_output, start_index)\n",
    "            start_features = tf.tile(start_features[None], [seq_len, 1, 1])\n",
    "            end_logits = self.end_logits_proj_layer0(\n",
    "                tf.concat([sequence_output, start_features], axis=-1))\n",
    "\n",
    "            end_logits = self.end_logits_layer_norm(end_logits)\n",
    "\n",
    "            end_logits = self.end_logits_proj_layer1(end_logits)\n",
    "            end_logits = tf.transpose(tf.squeeze(end_logits, -1), [1, 0])\n",
    "            end_logits_masked = end_logits * (1 - p_mask) - 1e30 * p_mask\n",
    "            end_log_probs = tf.nn.log_softmax(end_logits_masked, -1)\n",
    "        else:\n",
    "            start_top_log_probs, start_top_index = tf.nn.top_k(\n",
    "                start_log_probs, k=self.start_n_top)\n",
    "            start_index = tf.one_hot(\n",
    "                start_top_index, depth=seq_len, axis=-1, dtype=tf.float32)\n",
    "            start_features = tf.einsum(\n",
    "                'lbh,bkl->bkh', sequence_output, start_index)\n",
    "            end_input = tf.tile(sequence_output[:, :, None], [\n",
    "                                1, 1, self.start_n_top, 1])\n",
    "            start_features = tf.tile(start_features[None], [seq_len, 1, 1, 1])\n",
    "            end_input = tf.concat([end_input, start_features], axis=-1)\n",
    "            end_logits = self.end_logits_proj_layer0(end_input)\n",
    "            end_logits = tf.reshape(end_logits, [seq_len, -1, self.hidden_size])\n",
    "            end_logits = self.end_logits_layer_norm(end_logits)\n",
    "\n",
    "            end_logits = tf.reshape(end_logits,\n",
    "                                    [seq_len, -1, self.start_n_top, self.hidden_size])\n",
    "\n",
    "            end_logits = self.end_logits_proj_layer1(end_logits)\n",
    "            end_logits = tf.reshape(\n",
    "                end_logits, [seq_len, -1, self.start_n_top])\n",
    "            end_logits = tf.transpose(end_logits, [1, 2, 0])\n",
    "            end_logits_masked = end_logits * (\n",
    "                1 - p_mask[:, None]) - 1e30 * p_mask[:, None]\n",
    "            end_log_probs = tf.nn.log_softmax(end_logits_masked, -1)\n",
    "            end_top_log_probs, end_top_index = tf.nn.top_k(\n",
    "                end_log_probs, k=self.end_n_top)\n",
    "            end_top_log_probs = tf.reshape(end_top_log_probs,\n",
    "                                           [-1, self.start_n_top * self.end_n_top])\n",
    "            end_top_index = tf.reshape(end_top_index,\n",
    "                                       [-1, self.start_n_top * self.end_n_top])\n",
    "\n",
    "        # an additional layer to predict answerability\n",
    "\n",
    "        # get the representation of CLS\n",
    "        cls_index = tf.one_hot(cls_index, seq_len, axis=-1, dtype=tf.float32)\n",
    "        cls_feature = tf.einsum('lbh,bl->bh', sequence_output, cls_index)\n",
    "\n",
    "        # get the representation of START\n",
    "        start_p = tf.nn.softmax(start_logits_masked,\n",
    "                                axis=-1, name='softmax_start')\n",
    "        start_feature = tf.einsum('lbh,bl->bh', sequence_output, start_p)\n",
    "\n",
    "        ans_feature = tf.concat([start_feature, cls_feature], -1)\n",
    "        ans_feature = self.answer_class_proj_layer0(ans_feature)\n",
    "        ans_feature = self.ans_feature_dropout(\n",
    "            ans_feature, training=kwargs.get('training', False))\n",
    "        cls_logits = self.answer_class_proj_layer1(ans_feature)\n",
    "        cls_logits = tf.squeeze(cls_logits, -1)\n",
    "\n",
    "        if kwargs.get(\"training\", False):\n",
    "            return (start_log_probs, end_log_probs, cls_logits)\n",
    "        else:\n",
    "            return (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits)\n",
    "\n",
    "\n",
    "class ALBertQAModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, albert_config, max_seq_length, init_checkpoint, start_n_top, end_n_top, dropout=0.1, **kwargs):\n",
    "        super(ALBertQAModel, self).__init__(**kwargs)\n",
    "        self.albert_config = copy.deepcopy(albert_config)\n",
    "        self.initializer = tf.keras.initializers.TruncatedNormal(\n",
    "            stddev=self.albert_config.initializer_range)\n",
    "        float_type = tf.float32\n",
    "\n",
    "        input_word_ids = tf.keras.layers.Input(\n",
    "            shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "        input_mask = tf.keras.layers.Input(\n",
    "            shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "        input_type_ids = tf.keras.layers.Input(\n",
    "            shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "\n",
    "        albert_layer = AlbertModel(config=albert_config, float_type=float_type)\n",
    "\n",
    "        _, sequence_output = albert_layer(\n",
    "            input_word_ids, input_mask, input_type_ids)\n",
    "\n",
    "        self.albert_model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids],\n",
    "                                           outputs=[sequence_output])\n",
    "        if init_checkpoint != None:\n",
    "            self.albert_model.load_weights(init_checkpoint)\n",
    "\n",
    "        self.qalayer = ALBertQALayer(self.albert_config.hidden_size, start_n_top, end_n_top,\n",
    "                                     self.initializer, dropout)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # unpacked_inputs = tf_utils.unpack_inputs(inputs)\n",
    "        unique_ids = inputs[\"unique_ids\"]\n",
    "        input_word_ids = inputs[\"input_ids\"]\n",
    "        input_mask = inputs[\"input_mask\"]\n",
    "        segment_ids = inputs[\"segment_ids\"]\n",
    "        cls_index = tf.reshape(inputs[\"cls_index\"], [-1])\n",
    "        p_mask = inputs[\"p_mask\"]\n",
    "        if kwargs.get('training',False):\n",
    "            start_positions = inputs[\"start_positions\"]\n",
    "        else:\n",
    "            start_positions = None\n",
    "        sequence_output = self.albert_model(\n",
    "            [input_word_ids, input_mask, segment_ids], **kwargs)\n",
    "        output = self.qalayer(\n",
    "            sequence_output, p_mask, cls_index, start_positions, **kwargs)\n",
    "        return (unique_ids,) + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lower_case = True\n",
    "albert_config_file = \"xxlarge/config.json\"\n",
    "spm_model_file = \"xxlarge/vocab/30k-clean.model\"\n",
    "max_seq_length = 384\n",
    "\n",
    "model_dir = \"squad_out_v2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can have documents that are longer than the maximum sequence length.\n",
    "    # To deal with this we do a sliding window approach, where we take chunks\n",
    "    # of the up to our max length with a stride of `doc_stride`.\n",
    "_DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=None,spm_model_file=spm_model_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\n",
    "     For examples without an answer, the start and end position are -1.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               qas_id,\n",
    "               question_text,\n",
    "               paragraph_text,\n",
    "               orig_answer_text=None,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               is_impossible=False):\n",
    "    self.qas_id = qas_id\n",
    "    self.question_text = question_text\n",
    "    self.paragraph_text = paragraph_text\n",
    "    self.orig_answer_text = orig_answer_text\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.is_impossible = is_impossible\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.__repr__()\n",
    "\n",
    "  def __repr__(self):\n",
    "    s = \"\"\n",
    "    s += \"qas_id: %s\" % (tokenization.printable_text(self.qas_id))\n",
    "    s += \", question_text: %s\" % (\n",
    "        tokenization.printable_text(self.question_text))\n",
    "    s += \", paragraph_text: [%s]\" % (\" \".join(self.paragraph_text))\n",
    "    if self.start_position:\n",
    "      s += \", start_position: %d\" % (self.start_position)\n",
    "    if self.start_position:\n",
    "      s += \", end_position: %d\" % (self.end_position)\n",
    "    if self.start_position:\n",
    "      s += \", is_impossible: %r\" % (self.is_impossible)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               unique_id,\n",
    "               example_index,\n",
    "               doc_span_index,\n",
    "               tok_start_to_orig_index,\n",
    "               tok_end_to_orig_index,\n",
    "               token_is_max_context,\n",
    "               tokens,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               paragraph_len,\n",
    "               p_mask=None,\n",
    "               cls_index=None,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               is_impossible=None):\n",
    "    self.unique_id = unique_id\n",
    "    self.example_index = example_index\n",
    "    self.doc_span_index = doc_span_index\n",
    "    self.tok_start_to_orig_index = tok_start_to_orig_index\n",
    "    self.tok_end_to_orig_index = tok_end_to_orig_index\n",
    "    self.token_is_max_context = token_is_max_context\n",
    "    self.tokens = tokens\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.paragraph_len = paragraph_len\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.is_impossible = is_impossible\n",
    "    self.p_mask = p_mask\n",
    "    self.cls_index = cls_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_squad(question: str, paragraph: str):\n",
    "    qas_id = \"1234\"\n",
    "    orig_answer_text = None\n",
    "    start_position = None\n",
    "    is_impossible = None\n",
    "    examples = []\n",
    "    example = SquadExample(\n",
    "            qas_id=qas_id,\n",
    "            question_text=question,\n",
    "            paragraph_text=paragraph,\n",
    "            orig_answer_text=orig_answer_text,\n",
    "            start_position=start_position,\n",
    "            is_impossible=is_impossible)\n",
    "    examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "  # Because of the sliding window approach taken to scoring documents, a single\n",
    "  # token can appear in multiple documents. E.g.\n",
    "  #  Doc: the man went to the store and bought a gallon of milk\n",
    "  #  Span A: the man went to the\n",
    "  #  Span B: to the store and bought\n",
    "  #  Span C: and bought a gallon of\n",
    "  #  ...\n",
    "  #\n",
    "  # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "  # want to consider the score with \"maximum context\", which we define as\n",
    "  # the *minimum* of its left and right context (the *sum* of left and\n",
    "  # right context will always be the same, of course).\n",
    "  #\n",
    "  # In the example the maximum context for 'bought' would be span C since\n",
    "  # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "  # and 0 right context.\n",
    "  best_score = None\n",
    "  best_span_index = None\n",
    "  for (span_index, doc_span) in enumerate(doc_spans):\n",
    "    end = doc_span.start + doc_span.length - 1\n",
    "    if position < doc_span.start:\n",
    "      continue\n",
    "    if position > end:\n",
    "      continue\n",
    "    num_left_context = position - doc_span.start\n",
    "    num_right_context = end - position\n",
    "    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "    if best_score is None or score > best_score:\n",
    "      best_score = score\n",
    "      best_span_index = span_index\n",
    "\n",
    "  return cur_span_index == best_span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_index(index, pos, m=None, is_start=True):\n",
    "  \"\"\"Converts index.\"\"\"\n",
    "  if index[pos] is not None:\n",
    "    return index[pos]\n",
    "  n = len(index)\n",
    "  rear = pos\n",
    "  while rear < n - 1 and index[rear] is None:\n",
    "    rear += 1\n",
    "  front = pos\n",
    "  while front > 0 and index[front] is None:\n",
    "    front -= 1\n",
    "  assert index[front] is not None or index[rear] is not None\n",
    "  if index[front] is None:\n",
    "    if index[rear] >= 1:\n",
    "      if is_start:\n",
    "        return 0\n",
    "      else:\n",
    "        return index[rear] - 1\n",
    "    return index[rear]\n",
    "  if index[rear] is None:\n",
    "    if m is not None and index[front] < m - 1:\n",
    "      if is_start:\n",
    "        return index[front] + 1\n",
    "      else:\n",
    "        return m - 1\n",
    "    return index[front]\n",
    "  if is_start:\n",
    "    if index[rear] > index[front] + 1:\n",
    "      return index[front] + 1\n",
    "    else:\n",
    "      return index[rear]\n",
    "  else:\n",
    "    if index[rear] > index[front] + 1:\n",
    "      return index[rear] - 1\n",
    "    else:\n",
    "      return index[front]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training=False):\n",
    "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "  cnt_pos, cnt_neg = 0, 0\n",
    "  base_id = 1000000000\n",
    "  unique_id = base_id\n",
    "  max_n, max_m = 1024, 1024\n",
    "  f = np.zeros((max_n, max_m), dtype=np.float32)\n",
    "  \n",
    "  features = []\n",
    "\n",
    "  for (example_index, example) in enumerate(examples):\n",
    "\n",
    "    query_tokens = tokenization.encode_ids(\n",
    "        tokenizer.sp_model,\n",
    "        tokenization.preprocess_text(\n",
    "            example.question_text, lower=do_lower_case))\n",
    "\n",
    "    if len(query_tokens) > max_query_length:\n",
    "      query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "    paragraph_text = example.paragraph_text\n",
    "    para_tokens = tokenization.encode_pieces(\n",
    "        tokenizer.sp_model,\n",
    "        tokenization.preprocess_text(\n",
    "            example.paragraph_text, lower=do_lower_case),\n",
    "        return_unicode=False)\n",
    "\n",
    "    para_tokens_ = []\n",
    "    for para_token in para_tokens:\n",
    "      if type(para_token) == bytes:\n",
    "        para_token = para_token.decode(\"utf-8\")\n",
    "      para_tokens_.append(para_token)\n",
    "    para_tokens = para_tokens_\n",
    "\n",
    "    chartok_to_tok_index = []\n",
    "    tok_start_to_chartok_index = []\n",
    "    tok_end_to_chartok_index = []\n",
    "    char_cnt = 0\n",
    "    for i, token in enumerate(para_tokens):\n",
    "      chartok_to_tok_index.extend([i] * len(token))\n",
    "      tok_start_to_chartok_index.append(char_cnt)\n",
    "      char_cnt += len(token)\n",
    "      tok_end_to_chartok_index.append(char_cnt - 1)\n",
    "\n",
    "    tok_cat_text = \"\".join(para_tokens).replace(tokenization.SPIECE_UNDERLINE.decode(\"utf-8\"), \" \")\n",
    "    n, m = len(paragraph_text), len(tok_cat_text)\n",
    "\n",
    "    if n > max_n or m > max_m:\n",
    "      max_n = max(n, max_n)\n",
    "      max_m = max(m, max_m)\n",
    "      f = np.zeros((max_n, max_m), dtype=np.float32)\n",
    "\n",
    "    g = {}\n",
    "\n",
    "    def _lcs_match(max_dist, n=n, m=m):\n",
    "      \"\"\"Longest-common-substring algorithm.\"\"\"\n",
    "      f.fill(0)\n",
    "      g.clear()\n",
    "\n",
    "      ### longest common sub sequence\n",
    "      # f[i, j] = max(f[i - 1, j], f[i, j - 1], f[i - 1, j - 1] + match(i, j))\n",
    "      for i in range(n):\n",
    "\n",
    "        # note(zhiliny):\n",
    "        # unlike standard LCS, this is specifically optimized for the setting\n",
    "        # because the mismatch between sentence pieces and original text will\n",
    "        # be small\n",
    "        for j in range(i - max_dist, i + max_dist):\n",
    "          if j >= m or j < 0: continue\n",
    "\n",
    "          if i > 0:\n",
    "            g[(i, j)] = 0\n",
    "            f[i, j] = f[i - 1, j]\n",
    "\n",
    "          if j > 0 and f[i, j - 1] > f[i, j]:\n",
    "            g[(i, j)] = 1\n",
    "            f[i, j] = f[i, j - 1]\n",
    "\n",
    "          f_prev = f[i - 1, j - 1] if i > 0 and j > 0 else 0\n",
    "          if (tokenization.preprocess_text(\n",
    "              paragraph_text[i], lower=do_lower_case,\n",
    "              remove_space=False) == tok_cat_text[j]\n",
    "              and f_prev + 1 > f[i, j]):\n",
    "            g[(i, j)] = 2\n",
    "            f[i, j] = f_prev + 1\n",
    "\n",
    "    max_dist = abs(n - m) + 5\n",
    "    for _ in range(2):\n",
    "      _lcs_match(max_dist)\n",
    "      if f[n - 1, m - 1] > 0.8 * n: break\n",
    "      max_dist *= 2\n",
    "\n",
    "    orig_to_chartok_index = [None] * n\n",
    "    chartok_to_orig_index = [None] * m\n",
    "    i, j = n - 1, m - 1\n",
    "    while i >= 0 and j >= 0:\n",
    "      if (i, j) not in g: break\n",
    "      if g[(i, j)] == 2:\n",
    "        orig_to_chartok_index[i] = j\n",
    "        chartok_to_orig_index[j] = i\n",
    "        i, j = i - 1, j - 1\n",
    "      elif g[(i, j)] == 1:\n",
    "        j = j - 1\n",
    "      else:\n",
    "        i = i - 1\n",
    "\n",
    "    if (all(v is None for v in orig_to_chartok_index) or\n",
    "        f[n - 1, m - 1] < 0.8 * n):\n",
    "      logging.info(\"MISMATCH DETECTED!\")\n",
    "      continue\n",
    "\n",
    "    tok_start_to_orig_index = []\n",
    "    tok_end_to_orig_index = []\n",
    "    for i in range(len(para_tokens)):\n",
    "      start_chartok_pos = tok_start_to_chartok_index[i]\n",
    "      end_chartok_pos = tok_end_to_chartok_index[i]\n",
    "      start_orig_pos = _convert_index(chartok_to_orig_index, start_chartok_pos,\n",
    "                                      n, is_start=True)\n",
    "      end_orig_pos = _convert_index(chartok_to_orig_index, end_chartok_pos,\n",
    "                                    n, is_start=False)\n",
    "\n",
    "      tok_start_to_orig_index.append(start_orig_pos)\n",
    "      tok_end_to_orig_index.append(end_orig_pos)\n",
    "\n",
    "    if not is_training:\n",
    "      tok_start_position = tok_end_position = None\n",
    "\n",
    "    if is_training and example.is_impossible:\n",
    "      tok_start_position = 0\n",
    "      tok_end_position = 0\n",
    "\n",
    "    if is_training and not example.is_impossible:\n",
    "      start_position = example.start_position\n",
    "      end_position = start_position + len(example.orig_answer_text) - 1\n",
    "\n",
    "      start_chartok_pos = _convert_index(orig_to_chartok_index, start_position,\n",
    "                                         is_start=True)\n",
    "      tok_start_position = chartok_to_tok_index[start_chartok_pos]\n",
    "\n",
    "      end_chartok_pos = _convert_index(orig_to_chartok_index, end_position,\n",
    "                                       is_start=False)\n",
    "      tok_end_position = chartok_to_tok_index[end_chartok_pos]\n",
    "      assert tok_start_position <= tok_end_position\n",
    "\n",
    "    def _piece_to_id(x):\n",
    "      if six.PY2 and isinstance(x, six.text_type):\n",
    "        x = six.ensure_binary(x, \"utf-8\")\n",
    "      return tokenizer.sp_model.PieceToId(x)\n",
    "\n",
    "    all_doc_tokens = list(map(_piece_to_id, para_tokens))\n",
    "\n",
    "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "      length = len(all_doc_tokens) - start_offset\n",
    "      if length > max_tokens_for_doc:\n",
    "        length = max_tokens_for_doc\n",
    "      doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "      if start_offset + length == len(all_doc_tokens):\n",
    "        break\n",
    "      start_offset += min(length, doc_stride)\n",
    "\n",
    "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "      tokens = []\n",
    "      token_is_max_context = {}\n",
    "      segment_ids = []\n",
    "      p_mask = []\n",
    "      cls_index = 0\n",
    "\n",
    "      cur_tok_start_to_orig_index = []\n",
    "      cur_tok_end_to_orig_index = []\n",
    "\n",
    "      tokens.append(tokenizer.sp_model.PieceToId(\"[CLS]\"))\n",
    "      segment_ids.append(0)\n",
    "      p_mask.append(0)\n",
    "      for token in query_tokens:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "        p_mask.append(1)\n",
    "      tokens.append(tokenizer.sp_model.PieceToId(\"[SEP]\"))\n",
    "      segment_ids.append(0)\n",
    "      p_mask.append(1)\n",
    "\n",
    "      for i in range(doc_span.length):\n",
    "        split_token_index = doc_span.start + i\n",
    "\n",
    "        cur_tok_start_to_orig_index.append(\n",
    "            tok_start_to_orig_index[split_token_index])\n",
    "        cur_tok_end_to_orig_index.append(\n",
    "            tok_end_to_orig_index[split_token_index])\n",
    "\n",
    "        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                               split_token_index)\n",
    "        token_is_max_context[len(tokens)] = is_max_context\n",
    "        tokens.append(all_doc_tokens[split_token_index])\n",
    "        segment_ids.append(1)\n",
    "        p_mask.append(0)\n",
    "      tokens.append(tokenizer.sp_model.PieceToId(\"[SEP]\"))\n",
    "      segment_ids.append(1)\n",
    "      p_mask.append(1)\n",
    "\n",
    "      paragraph_len = len(tokens)\n",
    "      input_ids = tokens\n",
    "\n",
    "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "      # tokens are attended to.\n",
    "      input_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        p_mask.append(1)\n",
    "\n",
    "      assert len(input_ids) == max_seq_length\n",
    "      assert len(input_mask) == max_seq_length\n",
    "      assert len(segment_ids) == max_seq_length\n",
    "\n",
    "      span_is_impossible = example.is_impossible\n",
    "      start_position = None\n",
    "      end_position = None\n",
    "      if is_training and not span_is_impossible:\n",
    "        # For training, if our document chunk does not contain an annotation\n",
    "        # we throw it out, since there is nothing to predict.\n",
    "        doc_start = doc_span.start\n",
    "        doc_end = doc_span.start + doc_span.length - 1\n",
    "        out_of_span = False\n",
    "        if not (tok_start_position >= doc_start and\n",
    "                tok_end_position <= doc_end):\n",
    "          out_of_span = True\n",
    "        if out_of_span:\n",
    "          # continue\n",
    "          start_position = 0\n",
    "          end_position = 0\n",
    "          span_is_impossible = True\n",
    "        else:\n",
    "          doc_offset = len(query_tokens) + 2\n",
    "          start_position = tok_start_position - doc_start + doc_offset\n",
    "          end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "      if is_training and span_is_impossible:\n",
    "        start_position = cls_index\n",
    "        end_position = cls_index\n",
    "\n",
    "      if is_training:\n",
    "        feat_example_index = None\n",
    "      else:\n",
    "        feat_example_index = example_index\n",
    "\n",
    "      feature = InputFeatures(\n",
    "          unique_id=unique_id,\n",
    "          example_index=feat_example_index,\n",
    "          doc_span_index=doc_span_index,\n",
    "          tok_start_to_orig_index=cur_tok_start_to_orig_index,\n",
    "          tok_end_to_orig_index=cur_tok_end_to_orig_index,\n",
    "          token_is_max_context=token_is_max_context,\n",
    "          tokens=[tokenizer.sp_model.IdToPiece(x) for x in tokens],\n",
    "          input_ids=input_ids,\n",
    "          input_mask=input_mask,\n",
    "          segment_ids=segment_ids,\n",
    "          paragraph_len=paragraph_len,\n",
    "          start_position=start_position,\n",
    "          end_position=end_position,\n",
    "          is_impossible=span_is_impossible,\n",
    "          p_mask=p_mask,\n",
    "          cls_index=cls_index)\n",
    "\n",
    "      # Run callback\n",
    "      features.append(feature)\n",
    "    \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Victoria has a written constitution enacted in 1975, but based on the 1855 colonial constitution, passed by the United Kingdom Parliament as the Victoria Constitution Act 1855, which establishes the Parliament as the state's law-making body for matters coming under state responsibility. The Victorian Constitution can be amended by the Parliament of Victoria, except for certain 'entrenched' provisions that require either an absolute majority in both houses, a three-fifths majority in both houses, or the approval of the Victorian people in a referendum, depending on the provision.\"\n",
    "\n",
    "q = 'When did Victoria enact its constitution?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = convert_to_squad(q,doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[qas_id: 1234, question_text: When did Victoria enact its constitution?, paragraph_text: [V i c t o r i a   h a s   a   w r i t t e n   c o n s t i t u t i o n   e n a c t e d   i n   1 9 7 5 ,   b u t   b a s e d   o n   t h e   1 8 5 5   c o l o n i a l   c o n s t i t u t i o n ,   p a s s e d   b y   t h e   U n i t e d   K i n g d o m   P a r l i a m e n t   a s   t h e   V i c t o r i a   C o n s t i t u t i o n   A c t   1 8 5 5 ,   w h i c h   e s t a b l i s h e s   t h e   P a r l i a m e n t   a s   t h e   s t a t e ' s   l a w - m a k i n g   b o d y   f o r   m a t t e r s   c o m i n g   u n d e r   s t a t e   r e s p o n s i b i l i t y .   T h e   V i c t o r i a n   C o n s t i t u t i o n   c a n   b e   a m e n d e d   b y   t h e   P a r l i a m e n t   o f   V i c t o r i a ,   e x c e p t   f o r   c e r t a i n   ' e n t r e n c h e d '   p r o v i s i o n s   t h a t   r e q u i r e   e i t h e r   a n   a b s o l u t e   m a j o r i t y   i n   b o t h   h o u s e s ,   a   t h r e e - f i f t h s   m a j o r i t y   i n   b o t h   h o u s e s ,   o r   t h e   a p p r o v a l   o f   t h e   V i c t o r i a n   p e o p l e   i n   a   r e f e r e n d u m ,   d e p e n d i n g   o n   t h e   p r o v i s i o n .]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = convert_examples_to_features(examples,tokenizer,max_seq_length=384,doc_stride=128,max_query_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_config = AlbertConfig.from_json_file(albert_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_model = ALBertQAModel(albert_config, max_seq_length, init_checkpoint=None, start_n_top=5, end_n_top=5, dropout=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = tf.train.latest_checkpoint(model_dir)\n",
    "checkpoint = tf.train.Checkpoint(model=squad_model)\n",
    "checkpoint.restore(checkpoint_path).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
